services:
  # OpenTelemetry Collector - Central gateway for observability signals
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.102.1
    container_name: otel-collector
    restart: unless-stopped
    ports:
      # OTLP gRPC receiver
      - "4317:4317"
      # OTLP HTTP receiver
      - "4318:4318"
      # Prometheus metrics endpoint
      - "8888:8888"
      # Health check
      - "13133:13133"
    volumes:
      - ./otel-collector/config.yaml:/etc/otel-collector-config.yaml
      - ./certs:/etc/certs:ro
    environment:
      OTEL_CONFIG_PATH: /etc/otel-collector-config.yaml
    command: [ "--config=/etc/otel-collector-config.yaml" ]
    networks:
      - monitoring
    depends_on:
      - prometheus
      - loki
      - tempo
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:13133" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Prometheus - Metrics storage & query engine
  prometheus:
    image: prom/prometheus:v2.53.1
    container_name: prometheus
    restart: unless-stopped
    ports:
      # Prometheus web UI (internal only)
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/rules.yml:/etc/prometheus/rules.yml:ro
      - prometheus-storage:/prometheus
      - ./certs:/etc/certs:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - monitoring
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9090/-/healthy" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Loki - Log storage & query engine
  loki:
    image: grafana/loki:2.9.7
    container_name: loki
    restart: unless-stopped
    ports:
      # Loki API (internal only)
      - "3100:3100"
    volumes:
      - ./loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki-storage:/loki
      - loki-wal:/loki/wal
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - monitoring
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:3100/ready" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Tempo - Distributed tracing backend
  tempo:
    image: grafana/tempo:2.3.1
    container_name: tempo
    restart: unless-stopped
    ports:
      # Tempo API (internal only)
      - "3200:3200"
      # Jaeger receiver gRPC
      - "14250:14250"
      # Jaeger receiver HTTP
      - "14268:14268"
      # Zipkin receiver
      - "9411:9411"
    volumes:
      - ./tempo/tempo-config.yaml:/etc/tempo.yaml:ro
      - tempo-storage:/var/tempo
    command:
      - -config.file=/etc/tempo.yaml
    networks:
      - monitoring
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:3200/status/build" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Grafana - Visualization & alerting platform
  grafana:
    image: grafana/grafana:11.0.1
    container_name: grafana
    restart: unless-stopped
    ports:
      # Grafana web UI (internal only, use reverse proxy for public access)
      - "3000:3000"
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./dashboards:/var/lib/grafana/dashboards
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GF_ADMIN_PASSWORD:-ChangeMe123!@#}
      GF_USERS_ALLOW_SIGN_UP: 'false'
      GF_SECURITY_DISABLE_BRUTE_FORCE_LOGIN_PROTECTION: 'false'
      GF_SERVER_DOMAIN: ${GF_DOMAIN:-localhost}
      GF_SERVER_ROOT_URL: ${GF_ROOT_URL:-http://localhost:3000}
      GF_INSTALL_PLUGINS: 'grafana-clock-panel,grafana-simple-json-datasource,grafana-piechart-panel'
      GF_LOG_LEVEL: info
      GF_SMTP_ENABLED: ${GF_SMTP_ENABLED:-false}
      GF_SECURITY_COOKIE_SECURE: 'true'
      GF_SECURITY_COOKIE_HTTPONLY: 'true'
      GF_SECURITY_COOKIE_SAMESITE: 'Lax'
    networks:
      - monitoring
    depends_on:
      - prometheus
      - loki
      - tempo
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:3000/api/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

networks:
  monitoring:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  prometheus-storage:
    driver: local
  loki-storage:
    driver: local
  loki-wal:
    driver: local
  tempo-storage:
    driver: local
  grafana-storage:
    driver: local
