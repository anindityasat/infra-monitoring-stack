# Prometheus Alert Rules
# Reference: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/

groups:
  - name: system_alerts
    interval: 30s
    rules:
      # High CPU usage
      - alert: HighCPUUsage
        expr: |
          (1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100 > 80
        for: 5m
        labels:
          severity: warning
          category: infrastructure
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}% on instance {{ $labels.instance }}"

      # High memory usage
      - alert: HighMemoryUsage
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          category: infrastructure
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value }}% on instance {{ $labels.instance }}"

      # High disk usage
      - alert: HighDiskUsage
        expr: |
          (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lowerfs|squashfs|vfat"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lowerfs|squashfs|vfat"})) * 100 > 85
        for: 5m
        labels:
          severity: warning
          category: infrastructure
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage is {{ $value }}% on instance {{ $labels.instance }}"

      # High disk I/O
      - alert: HighDiskIOWait
        expr: |
          avg by (instance) (rate(node_cpu_seconds_total{mode="iowait"}[5m])) * 100 > 30
        for: 5m
        labels:
          severity: warning
          category: infrastructure
        annotations:
          summary: "High disk I/O wait on {{ $labels.instance }}"
          description: "Disk I/O wait is {{ $value }}% on instance {{ $labels.instance }}"

  - name: application_alerts
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
          category: application
        annotations:
          summary: "High error rate on {{ $labels.service }}"
          description: "Error rate is {{ $value }} req/s on service {{ $labels.service }}"

      # High latency
      - alert: HighLatency
        expr: |
          histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 1.0
        for: 5m
        labels:
          severity: warning
          category: application
        annotations:
          summary: "High latency on {{ $labels.service }}"
          description: "P99 latency is {{ $value }}s on service {{ $labels.service }}"

  - name: monitoring_stack_alerts
    interval: 30s
    rules:
      # Prometheus down
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
          category: monitoring
        annotations:
          summary: "Prometheus is down"
          description: "Prometheus instance is unreachable"

      # OpenTelemetry Collector down
      - alert: OTelCollectorDown
        expr: up{job="otel-collector"} == 0
        for: 1m
        labels:
          severity: critical
          category: monitoring
        annotations:
          summary: "OpenTelemetry Collector is down"
          description: "OTel Collector instance is unreachable"

      # Grafana down
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 1m
        labels:
          severity: critical
          category: monitoring
        annotations:
          summary: "Grafana is down"
          description: "Grafana instance is unreachable"
